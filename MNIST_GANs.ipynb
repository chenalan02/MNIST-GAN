{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52ef415-7e5f-4c54-9787-ed7bd897e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, ReLU, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ee7b91-0db5-4685-9989-96ff2e8dfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train / 255.0 * 2 - 1\n",
    "x_test = x_test / 255.0 * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accc4d40-26ef-473e-85b4-2c4180cf0c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d7dc34-5b9f-4306-9515-867ceb27752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, H, W = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5257d436-749d-4f75-a9d3-265327017335",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = H*W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de9a2c6-95c0-495c-afc1-29df61963d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, D)\n",
    "x_test  = x_test.reshape(-1, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5912cdf-afe2-4491-9d6a-d9dc78250486",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4611c04-9fb1-46c8-aa86-d67ff8d27bbf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 20:16:34.770871: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-01-14 20:16:34.770924: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: jupyter-alan-20chen\n",
      "2022-01-14 20:16:34.770938: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: jupyter-alan-20chen\n",
      "2022-01-14 20:16:34.771054: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.91.3\n",
      "2022-01-14 20:16:34.771082: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2022-01-14 20:16:34.771094: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.91.3\n",
      "2022-01-14 20:16:34.771594: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "i = Input(shape=(latent_dim,))\n",
    "x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
    "x = BatchNormalization(momentum=0.7)(x)\n",
    "x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
    "x = BatchNormalization(momentum=0.7)(x)\n",
    "x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
    "x = BatchNormalization(momentum=0.7)(x)\n",
    "x = Dense(D, activation='tanh')(x)\n",
    "\n",
    "generator = Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9fe452-c6ee-4146-9b86-457315db7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(D,))\n",
    "x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
    "x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfacb2da-31f9-4404-8ac4-1689b3c0c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(0.0002, 0.5),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b16b8a50-78ea-4ee4-adf4-acc35fd93a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "fake_pred = discriminator(img)\n",
    "combined_model = Model(z, fake_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a2bd3d8-59e8-4ace-af8d-5ea207e1423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c590e968-8e1c-4d1d-b7e1-6ad1869a2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 30000\n",
    "sample_period = 200\n",
    "\n",
    "ones = np.ones(batch_size)\n",
    "zeros = np.zeros(batch_size)\n",
    "\n",
    "if not os.path.exists('gan_images'):\n",
    "    os.makedirs('gan_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f5243ba-72e0-4615-b586-fda5dac59641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch):\n",
    "    rows, cols = 5, 5\n",
    "    noise = np.random.randn(rows * cols, latent_dim)\n",
    "    imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    imgs = 0.5 * imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols)\n",
    "    idx = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            axs[i, j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            idx += 1\n",
    "    fig.savefig(\"gan_images/%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3624552a-c989-48dd-9c2f-3aded982c3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 20:16:34.988791: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 200\n",
      "epoch: 400\n",
      "epoch: 600\n",
      "epoch: 800\n",
      "epoch: 1000\n",
      "epoch: 1200\n",
      "epoch: 1400\n",
      "epoch: 1600\n",
      "epoch: 1800\n",
      "epoch: 2000\n",
      "epoch: 2200\n",
      "epoch: 2400\n",
      "epoch: 2600\n",
      "epoch: 2800\n",
      "epoch: 3000\n",
      "epoch: 3200\n",
      "epoch: 3400\n",
      "epoch: 3600\n",
      "epoch: 3800\n",
      "epoch: 4000\n",
      "epoch: 4200\n",
      "epoch: 4400\n",
      "epoch: 4600\n",
      "epoch: 4800\n",
      "epoch: 5000\n",
      "epoch: 5200\n",
      "epoch: 5400\n",
      "epoch: 5600\n",
      "epoch: 5800\n",
      "epoch: 6000\n",
      "epoch: 6200\n",
      "epoch: 6400\n",
      "epoch: 6600\n",
      "epoch: 6800\n",
      "epoch: 7000\n",
      "epoch: 7200\n",
      "epoch: 7400\n",
      "epoch: 7600\n",
      "epoch: 7800\n",
      "epoch: 8000\n",
      "epoch: 8200\n",
      "epoch: 8400\n",
      "epoch: 8600\n",
      "epoch: 8800\n",
      "epoch: 9000\n",
      "epoch: 9200\n",
      "epoch: 9400\n",
      "epoch: 9600\n",
      "epoch: 9800\n",
      "epoch: 10000\n",
      "epoch: 10200\n",
      "epoch: 10400\n",
      "epoch: 10600\n",
      "epoch: 10800\n",
      "epoch: 11000\n",
      "epoch: 11200\n",
      "epoch: 11400\n",
      "epoch: 11600\n",
      "epoch: 11800\n",
      "epoch: 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 20:37:49.845655: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12200\n",
      "epoch: 12400\n",
      "epoch: 12600\n",
      "epoch: 12800\n",
      "epoch: 13000\n",
      "epoch: 13200\n",
      "epoch: 13400\n",
      "epoch: 13600\n",
      "epoch: 13800\n",
      "epoch: 14000\n",
      "epoch: 14200\n",
      "epoch: 14400\n",
      "epoch: 14600\n",
      "epoch: 14800\n",
      "epoch: 15000\n",
      "epoch: 15200\n",
      "epoch: 15400\n",
      "epoch: 15600\n",
      "epoch: 15800\n",
      "epoch: 16000\n",
      "epoch: 16200\n",
      "epoch: 16400\n",
      "epoch: 16600\n",
      "epoch: 16800\n",
      "epoch: 17000\n",
      "epoch: 17200\n",
      "epoch: 17400\n",
      "epoch: 17600\n",
      "epoch: 17800\n",
      "epoch: 18000\n",
      "epoch: 18200\n",
      "epoch: 18400\n",
      "epoch: 18600\n",
      "epoch: 18800\n",
      "epoch: 19000\n",
      "epoch: 19200\n",
      "epoch: 19400\n",
      "epoch: 19600\n",
      "epoch: 19800\n",
      "epoch: 20000\n",
      "epoch: 20200\n",
      "epoch: 20400\n",
      "epoch: 20600\n",
      "epoch: 20800\n",
      "epoch: 21000\n",
      "epoch: 21200\n",
      "epoch: 21400\n",
      "epoch: 21600\n",
      "epoch: 21800\n",
      "epoch: 22000\n",
      "epoch: 22200\n",
      "epoch: 22400\n",
      "epoch: 22600\n",
      "epoch: 22800\n",
      "epoch: 23000\n",
      "epoch: 23200\n",
      "epoch: 23400\n",
      "epoch: 23600\n",
      "epoch: 23800\n",
      "epoch: 24000\n",
      "epoch: 24200\n",
      "epoch: 24400\n",
      "epoch: 24600\n",
      "epoch: 24800\n",
      "epoch: 25000\n",
      "epoch: 25200\n",
      "epoch: 25400\n",
      "epoch: 25600\n",
      "epoch: 25800\n",
      "epoch: 26000\n",
      "epoch: 26200\n",
      "epoch: 26400\n",
      "epoch: 26600\n",
      "epoch: 26800\n",
      "epoch: 27000\n",
      "epoch: 27200\n",
      "epoch: 27400\n",
      "epoch: 27600\n",
      "epoch: 27800\n",
      "epoch: 28000\n",
      "epoch: 28200\n",
      "epoch: 28400\n",
      "epoch: 28600\n",
      "epoch: 28800\n",
      "epoch: 29000\n",
      "epoch: 29200\n",
      "epoch: 29400\n",
      "epoch: 29600\n",
      "epoch: 29800\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    real_imgs = x_train[idx]\n",
    "\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    fake_imgs = generator.predict(noise)\n",
    "\n",
    "    # train discriminator\n",
    "    discriminator.train_on_batch(real_imgs, ones)\n",
    "    discriminator.train_on_batch(fake_imgs, zeros)\n",
    "\n",
    "    # train generator\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    combined_model.train_on_batch(noise, ones)\n",
    "\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    combined_model.train_on_batch(noise, ones)\n",
    "\n",
    "    if epoch % sample_period == 0:\n",
    "        sample_images(epoch)\n",
    "        print(\"epoch: \"+str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f2a4c0-e793-45e0-8608-0220704071ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "model (Functional)           (None, 784)               1493520   \n",
      "_________________________________________________________________\n",
      "model_1 (Functional)         (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 2,027,025\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 537,089\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47bd3b0-39dc-4233-a9cd-c826a1457bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
